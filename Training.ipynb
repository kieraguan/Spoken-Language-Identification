{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieraguan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from dataloader import dataset_pipeline\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "# make a name for this experiment\n",
    "exp_name = 'project'  # change it as you like!\n",
    "\n",
    "# make a directory for the experiment\n",
    "if not os.path.exists(exp_name):\n",
    "    os.makedirs(exp_name)\n",
    "parser = argparse.ArgumentParser(description=exp_name)\n",
    "\n",
    "# add hyperparameters to the parser\n",
    "parser.add_argument('--batch-size', type=int, default=2000,\n",
    "                    help='input batch size for training (default: 2000)')\n",
    "parser.add_argument('--epochs', type=int, default=100,\n",
    "                    help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--freq-dim', type=int, default=24,\n",
    "                    help='the frequency dimension (default:24)')\n",
    "parser.add_argument('--cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training (default: False)')  # when you have a GPU\n",
    "parser.add_argument('--lr', type=float, default=4e-4,\n",
    "                    help='learning rate (default: 4e-4)')\n",
    "parser.add_argument('--model-save', type=str,  default='best_model.pt',\n",
    "                    help='path to save the best model')\n",
    "parser.add_argument('--tr-data', type=str,  default='tr.hdf5',\n",
    "                    help='path to training dataset')\n",
    "parser.add_argument('--val-data', type=str,  default='val.hdf5',\n",
    "                    help='path to validation dataset')\n",
    "parser.add_argument('--test-data', type=str,  default='test.hdf5',\n",
    "                    help='path to testing dataset')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "else:\n",
    "    kwargs = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_pipeline('Data/tr_data.hdf5'), \n",
    "                          batch_size=args.batch_size, \n",
    "                          shuffle=True, \n",
    "                          **kwargs)\n",
    "validation_loader = DataLoader(dataset_pipeline('Data/val_data.hdf5'),\n",
    "                               batch_size=args.batch_size, \n",
    "                               shuffle=False, \n",
    "                               **kwargs)\n",
    "test_loader = DataLoader(dataset_pipeline('Data/test_data.hdf5'), \n",
    "                         batch_size=args.batch_size, \n",
    "                         shuffle=False, \n",
    "                         **kwargs)\n",
    "args.dataset_len = len(train_loader)\n",
    "args.log_step = args.dataset_len // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layer1): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (activation1): ReLU()\n",
      "  (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer2): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=5, bias=True)\n",
      "    (1): Softmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "from model import MLP\n",
    "import torch.optim as optim\n",
    "model_MLP = MLP()\n",
    "print(model_MLP)\n",
    "\n",
    "# activate GPU if you have it\n",
    "if args.cuda:\n",
    "    model_MLP = model_MLP.cuda()\n",
    "# define the optimizer\n",
    "optimizer = optim.Adam(model_MLP.parameters(), lr=args.lr)\n",
    "# the scheduler is to define a exponential decay on the learning rate\n",
    "# every time the scheduler is activated, the learning rate is decayed by the given ratio\n",
    "# this is used to adjust the learning rate when the training plateaus \n",
    "scheduler  =optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_rnn(\n",
      "  (cov1): Conv2d(1, 16, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (cov2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (cov3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (cov4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (cov5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (bilstm): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5)\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=5, bias=True)\n",
      "    (1): Softmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import CNN_rnn\n",
    "import torch.optim as optim\n",
    "model_crnn =CNN_rnn()\n",
    "print(model_crnn)\n",
    "\n",
    "# activate GPU if you have it\n",
    "if args.cuda:\n",
    "    model_crnn = model_crnn.cuda()\n",
    "# define the optimizer\n",
    "optimizer = optim.Adam(model_crnn.parameters(), lr=args.lr)\n",
    "# the scheduler is to define a exponential decay on the learning rate\n",
    "# every time the scheduler is activated, the learning rate is decayed by the given ratio\n",
    "# this is used to adjust the learning rate when the training plateaus \n",
    "#scheduler  =optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)\n",
    "scheduler  =optim.lr_scheduler.StepLR(optimizer,step_size=11, gamma=0.5)\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss_func import loss_func\n",
    "from classcify import class_accuracy\n",
    "import time\n",
    "from train import train,validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "decay_cnt = 0\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    if args.cuda:\n",
    "        model_crnn.cuda()\n",
    "    training_loss.append(train(model_crnn, epoch,train_loader,args.cuda,optimizer,args.log_step))\n",
    "    validation_loss.append(validate(model_crnn, epoch,validation_loader,args.cuda))\n",
    "    \n",
    "    if training_loss[-1] == np.min(training_loss):\n",
    "        print('      Best training model found.')\n",
    "    if validation_loss[-1] == np.min(validation_loss):\n",
    "        # save current best model\n",
    "        with open(args.model_save, 'wb') as f:\n",
    "            torch.save(model_crnn.cpu().state_dict(), f)\n",
    "            print('      Best validation model found and saved.')\n",
    "    \n",
    "    print('-' * 99)\n",
    "    decay_cnt += 1\n",
    "    # lr decay\n",
    "    # decay when no best training model is found for 3 consecutive epochs\n",
    "    if np.min(training_loss) not in training_loss[-3:] and decay_cnt >= 3:\n",
    "        scheduler.step()\n",
    "        decay_cnt = 0\n",
    "        print('      Learning rate decreased.')\n",
    "        print('-' * 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
